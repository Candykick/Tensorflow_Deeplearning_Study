{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow-study-01.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sifdNcST_A9s",
        "yHl0pICilv42",
        "DWPnW1mqWqa6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1dtQbtP8wBe",
        "colab_type": "text"
      },
      "source": [
        "#텐서플로우로 시작하는 딥러닝 기초"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sifdNcST_A9s",
        "colab_type": "text"
      },
      "source": [
        "###Lab 02: Simple Linear Regression 를 TensorFlow 로 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4zhkQjg8k6i",
        "colab_type": "text"
      },
      "source": [
        "Build hypothesis and cost\n",
        "\n",
        "H(x) = Wx + b\n",
        "\n",
        "cost(W, b) = 차이의 제곱의 평균"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhC8WFiN8_RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "# Data\n",
        "x_data = [1,2,3,4,5]\n",
        "y_data = [1,2,3,4,5]\n",
        "\n",
        "# W, b initialize\n",
        "# 보통은 random값으로 지정한다.\n",
        "W = tf.Variable(2.9)\n",
        "b = tf.Variable(0.5)\n",
        "\n",
        "# learning_rate initialize\n",
        "# grad값을 얼마만큼 반영할지를 결정.\n",
        "# 보통 매우 작은 값을 사용함.\n",
        "learning_rate = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URZBTCeh-2_L",
        "colab_type": "text"
      },
      "source": [
        "minimize cost(W,b)를 만족하는 W, b 찾기\n",
        "\n",
        "(Gradient descent, 경사 하강법)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnIOUqhU-6yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2dfb61e4-68d3-4921-e3e7-c3edfd1e0dda"
      },
      "source": [
        "# 아래의 과정을 여러 차례 거쳐서 W, b값을 결정함.\n",
        "for i in range(100+1):\n",
        "  # Gradient descent\n",
        "  # 아래 블록의 변화를 tape에 기록.\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = W * x_data + b\n",
        "    # reduce_mean : 평균을 구하는 함수로, 평균을 구하면서 차원을 하나 낮춘다.\n",
        "    # 예) 1차원 배열을 넣는 경우 0차원의 값으로 결과가 나온다.\n",
        "    # 아래는 cost 함수.\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "  # cost 함수에서 W, b에 대한 개별 미분값을 순서대로 구함.\n",
        "  # 각각 W의 기울기, b의 기울기가 됨.\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "  # W값, b값을 업데이트\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  #10번째 시행마다 변화하는 값을 출력\n",
        "  if i % 10 == 0:\n",
        "    print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0|    2.4520|     0.376| 45.660004\n",
            "   10|    1.1036|  0.003398|  0.206336\n",
            "   20|    1.0128|  -0.02091|  0.001026\n",
            "   30|    1.0065|  -0.02184|  0.000093\n",
            "   40|    1.0059|  -0.02123|  0.000083\n",
            "   50|    1.0057|  -0.02053|  0.000077\n",
            "   60|    1.0055|  -0.01984|  0.000072\n",
            "   70|    1.0053|  -0.01918|  0.000067\n",
            "   80|    1.0051|  -0.01854|  0.000063\n",
            "   90|    1.0050|  -0.01793|  0.000059\n",
            "  100|    1.0048|  -0.01733|  0.000055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbW4bbdEFv6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a8381dff-faa8-4ff0-f32a-8e23b83729f6"
      },
      "source": [
        "# 5와 2.5를 x값으로 넣어서, y값이 각각 5와 2.5에 가깝게 나오는지 체크.\n",
        "print(W*5+b)\n",
        "print(W*2.5+b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.00667, shape=(), dtype=float32)\n",
            "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHl0pICilv42",
        "colab_type": "text"
      },
      "source": [
        "### Lab 03: Linear Regression and How to minimize cost 를 TensorFlow 로 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5I4im2qwVH3",
        "colab_type": "text"
      },
      "source": [
        "1. Numpy를 이용해서 Cost 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkCjqxD8l0K6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "16f42763-2c0f-4241-8c9d-7082297041cb"
      },
      "source": [
        "# Numpy만 가지고 구현해본 것.\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3])\n",
        "Y = np.array([1, 2, 3])\n",
        "\n",
        "# Cost 함수를 Python 함수로 구현.\n",
        "# Cost 함수는 (실제값과 함수값의 차이)^2의 평균.\n",
        "def cost_func(W, X, Y):\n",
        "  c = 0\n",
        "  for i in range(len(X)):\n",
        "    c += (W * X[i] - Y[i]) ** 2\n",
        "  return c / len(X)\n",
        "\n",
        "# -3 ~ 5사이를 15개의 구간으로 나누고, 이 값을 feed_W에 넣는다.\n",
        "# W값에 따라 Cost함수가 어떤 결과값을 가지는지 출력해본 것.\n",
        "for feed_W in np.linspace(-3, 5, num=15):\n",
        "  curr_cost = cost_func(feed_W, X, Y)\n",
        "  print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-3.000 |   74.66667\n",
            "-2.429 |   54.85714\n",
            "-1.857 |   38.09524\n",
            "-1.286 |   24.38095\n",
            "-0.714 |   13.71429\n",
            "-0.143 |    6.09524\n",
            " 0.429 |    1.52381\n",
            " 1.000 |    0.00000\n",
            " 1.571 |    1.52381\n",
            " 2.143 |    6.09524\n",
            " 2.714 |   13.71429\n",
            " 3.286 |   24.38095\n",
            " 3.857 |   38.09524\n",
            " 4.429 |   54.85714\n",
            " 5.000 |   74.66667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyFQItehwZ3K",
        "colab_type": "text"
      },
      "source": [
        "2. Tensorflow를 이용해서 Cost 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8kDog2Wu3tV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "7aa3fead-218d-4828-85b3-6929101deb57"
      },
      "source": [
        "# Tensorflow로 구현해본 것\n",
        "X = np.array([1, 2, 3])\n",
        "Y = np.array([1, 2, 3])\n",
        "\n",
        "# Cost 함수를 Python 함수로 구현.\n",
        "# Cost 함수는 (실제값과 함수값의 차이)^2의 평균.\n",
        "def cost_func(W, X, Y):\n",
        "  hypothesis = X * W #원래는 +b도 있지만 여기서는 b를 생략해서 간략화.\n",
        "  return tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "cost_values = []\n",
        "\n",
        "# -3 ~ 5사이를 15개의 구간으로 나누고, 이 값을 feed_W에 넣는다.\n",
        "# W값에 따라 Cost함수가 어떤 결과값을 가지는지 출력해본 것.\n",
        "for feed_W in np.linspace(-3, 5, num=15):\n",
        "  curr_cost = cost_func(feed_W, X, Y)\n",
        "  cost_values.append(curr_cost)\n",
        "  print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-3.000 |   74.66667\n",
            "-2.429 |   54.85714\n",
            "-1.857 |   38.09524\n",
            "-1.286 |   24.38095\n",
            "-0.714 |   13.71429\n",
            "-0.143 |    6.09524\n",
            " 0.429 |    1.52381\n",
            " 1.000 |    0.00000\n",
            " 1.571 |    1.52381\n",
            " 2.143 |    6.09524\n",
            " 2.714 |   13.71429\n",
            " 3.286 |   24.38095\n",
            " 3.857 |   38.09524\n",
            " 4.429 |   54.85714\n",
            " 5.000 |   74.66667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQB09-YYwdP7",
        "colab_type": "text"
      },
      "source": [
        "3. Gradient descent 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ff3FSZVwKL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "1f07a896-45d8-4e2c-ed86-1a86b0fee5d8"
      },
      "source": [
        "# Random 함수의 seed값을 초기화.\n",
        "# 다음에 이 코드를 다시 수행했을 때에도 동일하게 재현될 수 있도록 random seed를 특정 값을 줌.\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [1., 2., 3., 4.]\n",
        "\n",
        "# 정규분포를 따르는 random number를 1개 만듬.\n",
        "# W값이 얼마이던 동일한 결과로 수렴해 간다.\n",
        "W = tf.Variable(tf.random.normal([1], -100., 100.))\n",
        "\n",
        "# Gradient descent를 300번 수행\n",
        "for step in range(300):\n",
        "  # hypothesis, cost 함수 정의\n",
        "  hypothesis = W * x_data\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "  # 아래가 gradient descent.\n",
        "  alpha = 0.01\n",
        "  # gradient는 기울기로, Cost 함수의 미분값을 의미.\n",
        "  gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, x_data) - y_data, x_data))\n",
        "  # descent는 최종 값으로, W - a*기울기.\n",
        "  # 새로운 W값이기도 하다.\n",
        "  descent = W - tf.multiply(alpha, gradient)\n",
        "  # W값 업데이트.\n",
        "  W.assign(descent)\n",
        "\n",
        "  # 10번에 1번씩 cost, W값 출력\n",
        "  if step % 10 == 0:\n",
        "    print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | 18829.7812 |  47.348293\n",
            "   10 |  3959.8613 |  22.254509\n",
            "   20 |   832.7499 |  10.746943\n",
            "   30 |   175.1255 |   5.469776\n",
            "   40 |    36.8285 |   3.049760\n",
            "   50 |     7.7449 |   1.939984\n",
            "   60 |     1.6287 |   1.431060\n",
            "   70 |     0.3425 |   1.197676\n",
            "   80 |     0.0720 |   1.090651\n",
            "   90 |     0.0151 |   1.041571\n",
            "  100 |     0.0032 |   1.019064\n",
            "  110 |     0.0007 |   1.008742\n",
            "  120 |     0.0001 |   1.004009\n",
            "  130 |     0.0000 |   1.001839\n",
            "  140 |     0.0000 |   1.000843\n",
            "  150 |     0.0000 |   1.000387\n",
            "  160 |     0.0000 |   1.000178\n",
            "  170 |     0.0000 |   1.000081\n",
            "  180 |     0.0000 |   1.000037\n",
            "  190 |     0.0000 |   1.000017\n",
            "  200 |     0.0000 |   1.000008\n",
            "  210 |     0.0000 |   1.000004\n",
            "  220 |     0.0000 |   1.000002\n",
            "  230 |     0.0000 |   1.000001\n",
            "  240 |     0.0000 |   1.000001\n",
            "  250 |     0.0000 |   1.000001\n",
            "  260 |     0.0000 |   1.000001\n",
            "  270 |     0.0000 |   1.000001\n",
            "  280 |     0.0000 |   1.000001\n",
            "  290 |     0.0000 |   1.000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "587jHGKoySKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPnW1mqWqa6",
        "colab_type": "text"
      },
      "source": [
        "### Lab 04: Multi-variable Linear Regression 를 TensorFlow 로 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaMVIWHPMQM3",
        "colab_type": "text"
      },
      "source": [
        "기존의 방식으로, 행렬을 쓰지 않고 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJbSPeF2WwD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c6e65e0c-b9b4-45c1-b89c-b2a35e0465fd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# data and label\n",
        "x1 = [73., 93., 89., 96., 73.]\n",
        "x2 = [80., 88., 91., 98., 66.]\n",
        "x3 = [75., 93., 90., 100., 70.]\n",
        "Y = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# random weights\n",
        "w1 = tf.Variable(tf.random.normal([1]))\n",
        "w2 = tf.Variable(tf.random.normal([1]))\n",
        "w3 = tf.Variable(tf.random.normal([1]))\n",
        "b = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "  # tf.GradientTape() to record the gradient of the cost function\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = w1*x1 + w2*x2 + w3*x3 + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "  # calculates the gradients of the cost\n",
        "  w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "\n",
        "  # update w1, w2, w3 and b\n",
        "  w1.assign_sub(learning_rate * w1_grad)\n",
        "  w2.assign_sub(learning_rate * w2_grad)\n",
        "  w3.assign_sub(learning_rate * w3_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 50 == 0:\n",
        "    print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |   56661.3125\n",
            "   50 |     637.1641\n",
            "  100 |      15.4974\n",
            "  150 |       8.5788\n",
            "  200 |       8.4816\n",
            "  250 |       8.4601\n",
            "  300 |       8.4395\n",
            "  350 |       8.4189\n",
            "  400 |       8.3985\n",
            "  450 |       8.3781\n",
            "  500 |       8.3577\n",
            "  550 |       8.3373\n",
            "  600 |       8.3171\n",
            "  650 |       8.2969\n",
            "  700 |       8.2768\n",
            "  750 |       8.2566\n",
            "  800 |       8.2366\n",
            "  850 |       8.2167\n",
            "  900 |       8.1967\n",
            "  950 |       8.1768\n",
            " 1000 |       8.1570\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1qbXWFfMVor",
        "colab_type": "text"
      },
      "source": [
        "행렬을 써서 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDxU_BlqC4Rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "df454af5-dbbe-4772-cdd8-9729e688e2a0"
      },
      "source": [
        "# 데이터를 행렬에 넣음.\n",
        "data = np.array([\n",
        "                 # X1, X2, X3, y\n",
        "                 [73., 80., 75., 152.],\n",
        "                 [93., 88., 93., 185.],\n",
        "                 [89., 91., 90., 180.],\n",
        "                 [96., 98., 100., 196.],\n",
        "                 [73., 66., 70., 142.]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# 데이터를 slice. 이 때 X, Y 또한 행렬이 된다.\n",
        "# X는 input, Y는 label/출력.\n",
        "# [행(세로), 열(가로)]. ':-1'이면 마지막을 제외하고 전부 가져옴.\n",
        "# ':' 상태이면 처음부터 끝까지 가져옴.\n",
        "# [-1]이면 마지막만 가져옴.\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "# W는 가중치 3개가 들어간 행렬. 출력값은 1개.\n",
        "W = tf.Variable(tf.random.normal([3, 1]))\n",
        "# bias(Y절편)은 생략 가능.\n",
        "b = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "# matmul(arg1, arg2) 함수는 arg1 X arg2의 행렬곱을 수행한다.\n",
        "# bias(Y절편)은 생략 가능.\n",
        "def predict(X):\n",
        "  return tf.matmul(X, W) + b\n",
        "\n",
        "# Cost 최소값 찾기\n",
        "for i in range(2000+1):\n",
        "  # record the gradient of the cost function\n",
        "  with tf.GradientTape() as tape:\n",
        "    cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "  # calculates the gradients of the loss\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "  # updates parameters (W, b)\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |  4405.5991\n",
            "  100 |     3.5644\n",
            "  200 |     3.0082\n",
            "  300 |     2.9939\n",
            "  400 |     2.9799\n",
            "  500 |     2.9659\n",
            "  600 |     2.9520\n",
            "  700 |     2.9381\n",
            "  800 |     2.9244\n",
            "  900 |     2.9107\n",
            " 1000 |     2.8971\n",
            " 1100 |     2.8835\n",
            " 1200 |     2.8701\n",
            " 1300 |     2.8566\n",
            " 1400 |     2.8433\n",
            " 1500 |     2.8301\n",
            " 1600 |     2.8169\n",
            " 1700 |     2.8038\n",
            " 1800 |     2.7907\n",
            " 1900 |     2.7778\n",
            " 2000 |     2.7648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj3Eay8UQJEv",
        "colab_type": "text"
      },
      "source": [
        "**행렬을 썼을 때가 코딩의 양이 훨씬 적은 것을 볼 수 있다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb5pOgVMk8Fi",
        "colab_type": "text"
      },
      "source": [
        "### Lab 05-3: Logistic Regression/Classification 를 TensorFlow로 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2aeInrenj0j",
        "colab_type": "text"
      },
      "source": [
        "학습 데이터와 테스트 데이터 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN-aRC-hQLUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "00053b73-4a0b-42a7-e62b-1c88c4676c92"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# 학습 데이터.\n",
        "x_train = [[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]]\n",
        "y_train = [[0.], [0.], [0.], [1.], [1.], [1.]]\n",
        "\n",
        "# 테스트 데이터.\n",
        "x_test = [[5., 2.]]\n",
        "y_test = [[1.]]\n",
        "\n",
        "# x_train의 값에서 x1, x2 값 리스트를 가져옴.\n",
        "x1 = [x[0] for x in x_train] #1,2,3, ...\n",
        "x2 = [x[1] for x in x_train] #2,3,1, ...\n",
        "\n",
        "# scatter 차트로 보여줌. 축은 x1과 x2, 테스트값은 red로 표시.\n",
        "colors = [int(y[0]%3) for y in y_train]\n",
        "plt.scatter(x1, x2, c=colors, marker='^')\n",
        "colors = [int(y[0] % 3) for y in y_train]\n",
        "plt.scatter(x_test[0][0], x_test[0][1], c=\"red\")\n",
        "\n",
        "# 0의 값을 가지는 train_set은 보라색, 1의 값을 가지는 train_set은 노란색, test_set은 빨간색.\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ9CYXCsHpXD",
        "colab_type": "text"
      },
      "source": [
        "텐서플로를 이용해서 로지스틱 관계 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE--1X0anLlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋을 구성함. (0, 0)[0]의 형태로 6개가 들어가게 됨.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "# Weight, Bias를 초기화.\n",
        "W = tf.Variable(tf.zeros([2,1]), name='weight') # Weight는 2행 1열.\n",
        "b = tf.Variable(tf.zeros([1]), name='bias') # Bias는 상수.\n",
        "\n",
        "# 가설함수\n",
        "def logistic_regression(features):\n",
        "  # hypothesis는 1 / 1 + e^(-x)\n",
        "  # 여기서 x = tf.matmul(features, W) + b\n",
        "  hypothesis = tf.divide(1., 1. + tf.exp(-tf.matmul(features, W) + b))\n",
        "  return hypothesis\n",
        "\n",
        "# Loss Function = Cost Function\n",
        "def loss_fn(hypothesis, labels):\n",
        "  # cost(h(x), y) = -y*log(h(x)) - (1-y)log(1-h(x))\n",
        "  # cost값 자체는 cost(h(x), y)를 전부 계산한 뒤 reduce_mean을 수행한다.\n",
        "  cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "  return cost\n",
        "\n",
        "# learning_rate를 선언해서 cost값 줄이는 것을 선언한다.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "\n",
        "# 가설의 정확도를 측정하는 함\n",
        "def accuracy_fn(hypothesis, labels):\n",
        "  # predicted는 hypothesis > 0.5일 때 1, 아니면 0.\n",
        "  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "  # accuracy는 predicted와 labels이 얼마나 같은지를 계산한다.\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il5svCIsLgNM",
        "colab_type": "text"
      },
      "source": [
        "위에서 만든 함수로 실제 로지스틱 회귀 학습을 수행하고, 정확도 테스트하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYh5gVFto1nI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cd85b7e5-5212-4d13-b9c1-476c276ba502"
      },
      "source": [
        "# Gradient Descent를 구하는 함수.\n",
        "# W, b의 기울기를 반환한다.\n",
        "def grad(features, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = logistic_regression(features)\n",
        "    loss_value = loss_fn(hypothesis, labels)\n",
        "  return tape.gradient(loss_value, [W, b])\n",
        "\n",
        "EPOCHS = 1001\n",
        "for step in range(EPOCHS): # 학습을 1000번 한다.\n",
        "  for features, labels in iter(dataset.batch(len(x_train))): # 모든 데이터를 가져와서, 각 데이터를 1개씩 가져와서\n",
        "    hypothesis = logistic_regression(features) # 각 번째에 대한 x/y값을 가져와서 가설을 구함\n",
        "    grads = grad(features, labels) # 가설에 대한 W, b의 미분값을 구함\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b])) # 조금씩 cost(loss)의 최솟값을 향해 나아감\n",
        "    if step % 100 == 0:\n",
        "      print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(hypothesis, labels)))\n",
        "\n",
        "# 만들어진 모델을 테스트\n",
        "test_acc = accuracy_fn(logistic_regression(x_test), y_test) # x_test, y_test값을 넣어서 정상적으로 작동하는지 확인.\n",
        "print(\"Test Result = {}\".format(tf.cast(logistic_regression(x_test) > 0.5, dtype=tf.int32)))\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))\n",
        "# Testset Accuracy가 1이면 100%이므로 test case에 대해 잘 적용되었다는 의미."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.6931\n",
            "Iter: 100, Loss: 0.5781\n",
            "Iter: 200, Loss: 0.5352\n",
            "Iter: 300, Loss: 0.5056\n",
            "Iter: 400, Loss: 0.4840\n",
            "Iter: 500, Loss: 0.4673\n",
            "Iter: 600, Loss: 0.4537\n",
            "Iter: 700, Loss: 0.4421\n",
            "Iter: 800, Loss: 0.4320\n",
            "Iter: 900, Loss: 0.4229\n",
            "Iter: 1000, Loss: 0.4145\n",
            "Test Result = [[1]]\n",
            "Testset Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9AyW8cBN-qz",
        "colab_type": "text"
      },
      "source": [
        "### Lab 06-1: Softmax classifier 를 TensorFlow 로 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvM20DIHOuTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "312e5001-0247-4645-bc3e-8676c1155524"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# original data\n",
        "x_data = [[1,2,1,1],\n",
        "          [2,1,3,2],\n",
        "          [3,1,3,4],\n",
        "          [4,1,5,5],\n",
        "          [1,7,5,5],\n",
        "          [1,2,5,6],\n",
        "          [1,6,6,6],\n",
        "          [1,7,7,7]]\n",
        "y_data = [[0,0,1],\n",
        "          [0,0,1],\n",
        "          [0,0,1],\n",
        "          [0,1,0],\n",
        "          [0,1,0],\n",
        "          [0,1,0],\n",
        "          [1,0,0],\n",
        "          [1,0,0]]\n",
        "\n",
        "# convert original data into numpy with float format\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)\n",
        "\n",
        "# num classes\n",
        "nb_classes = 3\n",
        "\n",
        "# Weight와 Bias 구현.\n",
        "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight') # 4개의 특징값, class 3개. \n",
        "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
        "variables = [W, b]\n",
        "print(variables)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
            "array([[ 1.8717786 , -1.6804006 ,  2.3040414 ],\n",
            "       [-0.11599287,  0.1925582 ,  2.171322  ],\n",
            "       [ 0.9821109 , -0.02279125,  1.0290452 ],\n",
            "       [ 0.45549807,  1.4946007 ,  0.1486432 ]], dtype=float32)>, <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.11828838, -1.7362365 ,  0.63724595], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx01BCHIQ1Fg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fca23ce0-7f30-4c38-dacf-89cec7130abd"
      },
      "source": [
        "# 가설 함수(softmax 적용)\n",
        "def hypothesis(X):\n",
        "  hypo = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "  return hypo\n",
        "\n",
        "# Cost Function\n",
        "def cost_fn(X, Y):\n",
        "  logits = hypothesis(X)\n",
        "  # Cross Entropy Cost\n",
        "  cost = -tf.reduce_sum(Y * tf.math.log(logits), axis = 1)\n",
        "  cost_mean = tf.reduce_mean(cost)\n",
        "  return cost_mean\n",
        "\n",
        "print(cost_fn(x_data, y_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(9.18342, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m-9UbPkUneo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "63c0440d-5f91-4590-bcfa-2c0c113a05ab"
      },
      "source": [
        "# Gradient Function\n",
        "def grad_fn(X, Y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    cost = cost_fn(X, Y)\n",
        "    grads = tape.gradient(cost, variables)\n",
        "    return grads\n",
        "\n",
        "print(grad_fn(x_data, y_data))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
            "array([[-0.20068437, -0.7499256 ,  0.95061   ],\n",
            "       [-1.6033776 , -1.2498528 ,  2.8532305 ],\n",
            "       [-1.5563188 , -1.8746539 ,  3.4309726 ],\n",
            "       [-1.5529563 , -1.9995863 ,  3.5525427 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.23145336, -0.3749262 ,  0.6063795 ], dtype=float32)>]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keuFMXi0jocY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6cdcf837-5d4a-40c6-f46f-77d8f40e2f7a"
      },
      "source": [
        "# Train Function\n",
        "def fit(X, Y, epochs=2000, verbose=100):\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "  for i in range(epochs):\n",
        "    grads = grad_fn(X, Y)\n",
        "    optimizer.apply_gradients(zip(grads, variables))\n",
        "    if (i==0) | ((i+1)%verbose==0):\n",
        "      print('Loss at epoch %d: %f'%(i+1, cost_fn(X, Y).numpy()))\n",
        "\n",
        "fit(x_data, y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch 1: 4.981570\n",
            "Loss at epoch 100: 0.630812\n",
            "Loss at epoch 200: 0.588680\n",
            "Loss at epoch 300: 0.539884\n",
            "Loss at epoch 400: 0.492932\n",
            "Loss at epoch 500: 0.447175\n",
            "Loss at epoch 600: 0.401864\n",
            "Loss at epoch 700: 0.356551\n",
            "Loss at epoch 800: 0.311180\n",
            "Loss at epoch 900: 0.267089\n",
            "Loss at epoch 1000: 0.235355\n",
            "Loss at epoch 1100: 0.223064\n",
            "Loss at epoch 1200: 0.212659\n",
            "Loss at epoch 1300: 0.203139\n",
            "Loss at epoch 1400: 0.194398\n",
            "Loss at epoch 1500: 0.186346\n",
            "Loss at epoch 1600: 0.178906\n",
            "Loss at epoch 1700: 0.172014\n",
            "Loss at epoch 1800: 0.165613\n",
            "Loss at epoch 1900: 0.159652\n",
            "Loss at epoch 2000: 0.154090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK-P8HeojpNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "87813402-b0a3-4987-8a45-0fb4945f0dd8"
      },
      "source": [
        "# Prediction\n",
        "a = hypothesis(x_data)\n",
        "\n",
        "print(a)\n",
        "print(tf.argmax(a, 1))\n",
        "print(tf.argmax(y_data, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[2.4521084e-06 7.9871702e-04 9.9919885e-01]\n",
            " [1.6302433e-03 7.7675231e-02 9.2069453e-01]\n",
            " [5.0869062e-08 1.5900762e-01 8.4099227e-01]\n",
            " [1.1994345e-06 8.5567892e-01 1.4431983e-01]\n",
            " [2.5495759e-01 7.3340619e-01 1.1636280e-02]\n",
            " [1.3434932e-01 8.6564642e-01 4.2568681e-06]\n",
            " [7.5521117e-01 2.4477372e-01 1.5105590e-05]\n",
            " [9.1835761e-01 8.1642210e-02 2.2198495e-07]], shape=(8, 3), dtype=float32)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNgEAaNlvEC",
        "colab_type": "text"
      },
      "source": [
        "### Lab 06-2: Fancy Softmax classifier 를 TensorFlow 로 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL33e4ynkLt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 데이터 받아오기\n",
        "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1] #마지막 세로줄 제외 선택\n",
        "y_data = xy[:, [-1]] #마지막 세로줄 선택"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gC9DkQ68qbA",
        "colab_type": "text"
      },
      "source": [
        "one-hot에 대한 설명\n",
        "<br>- 인덱스에 해당하는 위치에 1, 그렇지 않으면 0으로 표기한다.\n",
        "<br>- argmax와 함께 자주 사용된다.\n",
        "<br>- 원래 행렬보다 한 차원 증가한 결과값이 나온다.\n",
        "<br>\n",
        "<br>\n",
        "함수 원형은 아래와 같다.<br>\n",
        "tf.one_hot(\n",
        "    indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None\n",
        ")\n",
        "<br><br>\n",
        "예시\n",
        "<br>\n",
        "tf.one_hot([0, 1, 2], depth=3).eval(session=sess)<br>\n",
        "=> array([[1., 0., 0.],[0., 1., 0.],[0., 0., 1.]], dtype=float32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5HtDfQMmKEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b18319a-dac5-487b-aecf-2aca7de7818a"
      },
      "source": [
        "nb_classes = 7 #0~6\n",
        "\n",
        "# 데이터 전처리. Y값을 one-hot 형태로 변환.\n",
        "# y_data의 차원을 하나 늘린다고 생각하면 된다.\n",
        "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes) #shape = (?, 1, 7)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) #shape = (?, 7)\n",
        "\n",
        "print(x_data.shape, Y_one_hot.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101, 16) (101, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n13gQKmkmomt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight / Bias Setting\n",
        "W = tf.Variable(tf.random.normal([16, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
        "variables = [W, b]\n",
        "\n",
        "# logistic function\n",
        "# softmax_cross_entropy_with_logits 함수를 쓰기 위함\n",
        "def logit_fn(X):\n",
        "  return tf.matmul(X, W) + b\n",
        "\n",
        "# hypothesis function\n",
        "# 정확도 측정을 위한 함수\n",
        "def hypothesis(X):\n",
        "  return tf.nn.softmax(logit_fn(X))\n",
        "\n",
        "# cost function\n",
        "def cost_fn(X, Y):\n",
        "  logits = logit_fn(X)\n",
        "  cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, from_logits=True)  \n",
        "\n",
        "  cost = tf.reduce_mean(cost_i)\n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkl8-QtKpYvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradient function\n",
        "def grad_fn(X, Y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = cost_fn(X, Y)\n",
        "        grads = tape.gradient(loss, variables)\n",
        "        return grads\n",
        "\n",
        "# prediction function\n",
        "# 예측의 정확도 리턴\n",
        "def prediction(X, Y):\n",
        "    pred = tf.argmax(hypothesis(X), 1)\n",
        "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFPCqyINpxZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8e520be9-338b-48bf-ecdd-e153d9e7fe6d"
      },
      "source": [
        "# 학습 함수\n",
        "# 06-1과 달리 예측의 정확도도 출력함.\n",
        "def fit(X, Y, epochs=500, verbose=50):\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "  for i in range(epochs):\n",
        "    grads = grad_fn(X, Y)\n",
        "    optimizer.apply_gradients(zip(grads, variables))\n",
        "\n",
        "    if (i==0) | ((i+1)%verbose==0):\n",
        "      acc = prediction(X, Y).numpy()\n",
        "      loss = tf.reduce_sum(cost_fn(X, Y)).numpy()\n",
        "\n",
        "      print('Loss & Acc at {} epoch {}, {}'.format(i+1, loss, acc))\n",
        "\n",
        "\n",
        "fit(x_data, Y_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss & Acc at 1 epoch 9.152242660522461, 0.11881188303232193\n",
            "Loss & Acc at 50 epoch 1.3705426454544067, 0.5643564462661743\n",
            "Loss & Acc at 100 epoch 0.7501804828643799, 0.7722772359848022\n",
            "Loss & Acc at 150 epoch 0.5672702789306641, 0.8217821717262268\n",
            "Loss & Acc at 200 epoch 0.46701985597610474, 0.8811880946159363\n",
            "Loss & Acc at 250 epoch 0.39931121468544006, 0.9009901285171509\n",
            "Loss & Acc at 300 epoch 0.3491809070110321, 0.9306930899620056\n",
            "Loss & Acc at 350 epoch 0.3100760579109192, 0.9306930899620056\n",
            "Loss & Acc at 400 epoch 0.2784937620162964, 0.9405940771102905\n",
            "Loss & Acc at 450 epoch 0.2523317039012909, 0.9405940771102905\n",
            "Loss & Acc at 500 epoch 0.2302367389202118, 0.9405940771102905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPox4HlJzRYi",
        "colab_type": "text"
      },
      "source": [
        "### Lab 07-2-2: Application & Tips: 학습률, 전처리, 오버피팅을 TensorFlow 로 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR0FCWihzSWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d77cb60c-b600-4106-affd-87c4f2b28aa8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 데이터 전처리 : Normalization이 필요하다.\n",
        "# 중간 중간에 엄청나게 큰 데이터들이 있다. -> 이런 데이터는 없애야 한다.\n",
        "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
        "\n",
        "# 데이터 정규화\n",
        "def normalization(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    return numerator / denominator\n",
        "\n",
        "xy = normalization(xy)\n",
        "x_train = xy[:, 0:-1]\n",
        "y_train = xy[:, [-1]]\n",
        "print(xy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         1.         0.         1.         1.        ]\n",
            " [0.70548491 0.70439552 1.         0.71881783 0.83755792]\n",
            " [0.54412549 0.50274824 0.57608696 0.60646801 0.6606331 ]\n",
            " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
            " [0.51436    0.4258239  0.30434783 0.58504805 0.42624401]\n",
            " [0.49556179 0.4258239  0.31521739 0.48131134 0.49276137]\n",
            " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
            " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgKohPR0EDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋 선언\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
        "\n",
        "# Weight, Bias 초기화\n",
        "W = tf.Variable(tf.random.normal([4, 1]), dtype=tf.float32)\n",
        "b = tf.Variable(tf.random.normal([1]), dtype=tf.float32)\n",
        "\n",
        "# Hypothesis function\n",
        "def linearReg_fn(features):\n",
        "  hypothesis = tf.matmul(features, W) + b\n",
        "  return hypothesis\n",
        "\n",
        "# Overfitting 방지를 위한 function\n",
        "def l2_loss(loss, beta = 0.01):\n",
        "  W_reg = tf.nn.l2_loss(W)\n",
        "  loss = tf.reduce_mean(loss + W_reg * beta)\n",
        "  return loss\n",
        "\n",
        "# Cost Function\n",
        "# flag가 True인 경우 l2_loss를 적용\n",
        "def loss_fn(hypothesis, features, labels, flag = False):\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - labels))\n",
        "    if(flag):\n",
        "        cost = l2_loss(cost)\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WzvmZf72HzU",
        "colab_type": "text"
      },
      "source": [
        "##### 5개 파라미터 설정\n",
        "- starter_learning_rate : 최초 학습시 사용될 learning rate (0.1로 설정하여 0.96씩 감소하는지 확인)\n",
        "- global_step : 현재 학습 횟수\n",
        "- 1000 : 곱할 횟수 정의 (1000번에 마다 적용)\n",
        "- 0.96 : 기존 learning에 곱할 값\n",
        "- 적용유무 decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
        "- decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH9tBIAv1wQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_decay = True\n",
        "starter_learning_rate = 0.1\n",
        "\n",
        "if(is_decay):    \n",
        "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=starter_learning_rate,\n",
        "                                                                  decay_steps=50,\n",
        "                                                                  decay_rate=0.96,\n",
        "                                                                  staircase=True)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "else:\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=starter_learning_rate)\n",
        "\n",
        "def grad(hypothesis, features, labels, l2_flag):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(linearReg_fn(features),features,labels, l2_flag)\n",
        "    return tape.gradient(loss_value, [W,b]), loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ywE3PyM2fk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9e15d8d4-88fc-4423-9ba3-ea392716ad6f"
      },
      "source": [
        "EPOCHS = 101\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in dataset:\n",
        "        features = tf.cast(features, tf.float32) #type 맞춰줌\n",
        "        labels = tf.cast(labels, tf.float32) #type 맞춰줌\n",
        "        grads, loss_value = grad(linearReg_fn(features), features, labels, False)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))        \n",
        "    if step % 10 == 0:\n",
        "        print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.0457\n",
            "Iter: 10, Loss: 0.0347\n",
            "Iter: 20, Loss: 0.0268\n",
            "Iter: 30, Loss: 0.0210\n",
            "Iter: 40, Loss: 0.0167\n",
            "Iter: 50, Loss: 0.0135\n",
            "Iter: 60, Loss: 0.0112\n",
            "Iter: 70, Loss: 0.0094\n",
            "Iter: 80, Loss: 0.0081\n",
            "Iter: 90, Loss: 0.0071\n",
            "Iter: 100, Loss: 0.0064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxzFHZ1P72Bx",
        "colab_type": "text"
      },
      "source": [
        "### Lab 07-3-2: Application & Tips: 다양한 Dataset 으로 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UBlCDCL8yX_",
        "colab_type": "text"
      },
      "source": [
        "#### Fashion MNIST-Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aME5SXA378jg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d14dd9a4-cf7f-4b59-fc73-e5fc39c353c2"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Tensorflow Code\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT6NH2gm8OvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2a63bf0c-8f09-4f65-f84f-6da9d9ef226f"
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "train_images = train_images / 255.0 # (60000, 28, 28)\n",
        "test_images = test_images / 255.0 #(10000, 28, 28)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
        "                             tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                             tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "predictions = model.predict(test_images)\n",
        "np.argmax(predictions[0]) # 9 label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5019 - accuracy: 0.8240\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3758 - accuracy: 0.8645\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3337 - accuracy: 0.8783\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3120 - accuracy: 0.8861\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2948 - accuracy: 0.8918\n",
            "313/313 [==============================] - 0s 992us/step - loss: 0.3442 - accuracy: 0.8739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j-y6l-09Sro",
        "colab_type": "text"
      },
      "source": [
        "#### IMDB-Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQfSHMLS8mYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c2928769-3628-42cb-c474-b8e661e56095"
      },
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2 # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(train_data[4])\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding='post', maxlen=256)\n",
        "\n",
        "vocab_size = 10000\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 16))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hui5meJU9WBZ",
        "colab_type": "text"
      },
      "source": [
        "#### CIFAR-100\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl2HjOa599Cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54f34415-1ed0-41a7-ec5b-55dc6fd1dda2"
      },
      "source": [
        " from keras.datasets import cifar100\n",
        " (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}